{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c274512-12c8-404c-8aff-4730566d2c50",
   "metadata": {},
   "source": [
    "## Event-Driven revolution for Robotic Perception ðŸ¤–\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"Images/Slide2.png\" alt=\"image_description\" width=\"600\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "**In an unconstrained world, autonomous agents like robots need low latency and power consumption, requiring optimal performance with compact and efficient hardware in limited space. Meeting the demand for faster systems, smaller devices, and accurate responses is a complex challenge. Bioinspiration and in-memory computing offer alternatives to traditional architectures. The combination of event-driven sensing with its microsecond-level latency; and parallel computation, where a hundred billion neurons consume a mere 20 watts (equivalent to an energy-saving light bulb), presents a highly promising technology. I will provide an overview of these technologies, discussing their potential and limitations, backed by studies conducted at the Event Driven Perception for Robotics laboratory.***\n",
    "\n",
    "## Introduction ðŸ“š\n",
    "\n",
    "Event-driven cameras, also known as neuromorphic or asynchronous cameras, are a type of imaging sensor that operates in a fundamentally different way compared to traditional cameras. Instead of capturing images at fixed intervals, event-driven cameras only capture pixel-level changes in the scene, resulting in a stream of asynchronous events. \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"Images/Slide12.png\" alt=\"image_description\" width=\"600\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "This unique approach offers several advantages and has gained significant interest in various fields, including computer vision, robotics, and artificial intelligence.\n",
    "In this notebook, we will delve into the capabilities and functionalities of event-driven cameras, as well as their bioinspiration from the human retina.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"Images/example.gif\" alt=\"\" width=\"300\" height=\"200\" />\n",
    "   \n",
    "    Asynchronous stream of events from Event-Driven (ED) cameras. Green and pink are positive and negative polarity, in yellow the overlap of polarities.\n",
    "</div>\n",
    "\n",
    "## Bioinspiration from the Human Retina ðŸ§ \n",
    "\n",
    "Event-driven cameras draw inspiration from the functioning of the human retina, which is responsible for converting light into neural signals. The retina employs a similar event-driven approach, where photoreceptor cells transmit electrical signals only when there is a change in the light intensity falling on them.\n",
    "\n",
    "This bioinspiration allows event-driven cameras to mimic the efficiency and capabilities of the human retina. By capturing only changes in the scene, event-driven cameras reduce redundant information and improve the overall efficiency of visual processing systems. Additionally, they enable low-latency, high-speed, and low-power vision applications, replicating the key features of the human visual system.\n",
    "\n",
    "## Event-Driven Camera Capabilities ðŸŽ¥\n",
    "\n",
    "Event-driven cameras possess several compelling capabilities that set them apart from traditional cameras. Let's explore some of these capabilities:\n",
    "\n",
    "1. **High Temporal Resolution**: Event-driven cameras offeran  extremely high temporal resolution, enabling them to capture events with sub-millisecond accuracy. This makes them suitable for applications requiring precise timing, such as tracking fast-moving objects or capturing rapid changes in the scene.\n",
    "\n",
    "2. **High Dynamic Range**: Event-driven cameras have an inherent high dynamic range, allowing them to handle scenes with a wide range of lighting conditions. Unlike traditional cameras, event-driven cameras can simultaneously capture both dim and bright parts of the scene without saturation, providing rich details across different illumination levels.\n",
    "\n",
    "3. **Low Power Consumption**: Event-driven cameras consume significantly less power compared to traditional cameras. This is because they only transmit data when there is a change in the scene, resulting in sparse data transmission. As a result, event-driven cameras are suitable for applications with power-constrained devices, such as mobile robotics or drones.\n",
    "\n",
    "4. **Low Latency**: Due to their asynchronous nature, event-driven cameras exhibit extremely low latency. Events are transmitted almost instantaneously as they occur, allowing real-time processing and analysis of the captured data. This capability is beneficial for applications that require quick responses, such as object tracking or collision avoidance.\n",
    "\n",
    "5. **Robustness to Motion Blur**: Traditional cameras suffer from motion blur when capturing fast-moving objects. Event-driven cameras, on the other hand, excel at capturing motion without blur due to their high temporal resolution. This makes them ideal for applications involving fast motion analysis, such as sports analytics or autonomous vehicles.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"Images/Slide13.png\" alt=\"image_description\" width=\"600\" height=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70bfe-9e4a-4593-95a4-4c3b92c54dc3",
   "metadata": {},
   "source": [
    "## Let's start with a mini event-driven tutorial ðŸŽ‰\n",
    "\n",
    "You will learn how to load real event-driven data and visualise them in a window! \n",
    "\n",
    "The data you will load represent a bottle and two mugs on a desk. The data come from the ATIS event-based camera with a resolution of 304x240."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d4aa3a-836b-422f-a29b-1aa77418c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bimvee==1.0.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.0.17)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bimvee==1.0.17) (65.5.0)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bimvee==1.0.17) (3.7.1)\n",
      "Requirement already satisfied: imageio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bimvee==1.0.17) (2.31.0)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bimvee==1.0.17) (0.12.2)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bimvee==1.0.17) (4.7.0.72)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bimvee==1.0.17) (1.24.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bimvee==1.0.17) (4.65.0)\n",
      "Requirement already satisfied: hickle in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bimvee==1.0.17) (5.0.2)\n",
      "Requirement already satisfied: h5py>=2.10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from hickle->bimvee==1.0.17) (3.8.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from imageio->bimvee==1.0.17) (9.5.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bimvee==1.0.17) (4.39.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bimvee==1.0.17) (23.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bimvee==1.0.17) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bimvee==1.0.17) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bimvee==1.0.17) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bimvee==1.0.17) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->bimvee==1.0.17) (1.0.7)\n",
      "Requirement already satisfied: pandas>=0.25 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from seaborn->bimvee==1.0.17) (2.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=0.25->seaborn->bimvee==1.0.17) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=0.25->seaborn->bimvee==1.0.17) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->bimvee==1.0.17) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bimvee==1.0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e797c908-b039-4bb0-8e5e-0187d6fc7869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importIitYarp trying path: data\n",
      "importIitYarp trying path: data/.ipynb_checkpoints\n",
      "    \"data.log\" file not found\n",
      "Examining info.log: data/info.log\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "from bimvee.importIitYarp import importIitYarp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "events = importIitYarp(filePathOrName=\"data\", tsBits=30)\n",
    "e_x = events['data']['left']['dvs']['x']\n",
    "e_y = events['data']['left']['dvs']['y']\n",
    "e_ts = np.multiply(events['data']['left']['dvs']['ts'], 10**3)\n",
    "e_pol = events['data']['left']['dvs']['pol']\n",
    "\n",
    "width = 304\n",
    "height = 240\n",
    "window_period = 1 #ms\n",
    "window = np.zeros((height,width))\n",
    "\n",
    "for x, y, ts, pol in zip(e_x, e_y, e_ts, e_pol):\n",
    "    if ts<=window_period:\n",
    "        window[y][x]=1\n",
    "    else:\n",
    "        plt.imshow(window)\n",
    "        plt.draw()\n",
    "        plt.pause(0.2)\n",
    "        matrix_events = np.zeros((height, width))\n",
    "        window_period += window_period\n",
    "        window = np.zeros((height, width))\n",
    "\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527cf91-aaab-41ab-896f-8361617b4323",
   "metadata": {},
   "source": [
    "\n",
    "## Event-driven sensing and Neuromorphic platforms ðŸ’µ\n",
    "\n",
    "Neuromorphic platforms are specialized computing systems that aim to mimic the structure and functionality of the human brain. These platforms leverage event-driven sensing and event-driven cameras as key components to capture and process information in a manner inspired by the human visual system.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"Images/Slide26.png\" alt=\"image_description\" width=\"600\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "By leveraging event-driven sensing and event-driven cameras, neuromorphic platforms can replicate the efficiency and capabilities of the human visual system. These platforms are designed to process and analyze the captured events using neuromorphic computing architectures, which utilize algorithms and circuitry inspired by the structure and functioning of neural networks.\n",
    "\n",
    "Neuromorphic platforms offer several advantages over traditional computing systems:\n",
    "\n",
    "1. **Low Power Consumption**: Event-driven sensing and processing enable energy-efficient operations. By transmitting and processing data only when significant changes occur, neuromorphic platforms reduce power consumption, making them suitable for resource-constrained devices and applications.\n",
    "\n",
    "2. **Real-Time Processing**: Event-driven cameras provide low-latency and high temporal resolution, allowing neuromorphic platforms to process information in real-time. This capability is valuable in applications that require quick responses and real-time decision-making, such as robotics, autonomous vehicles, and augmented reality.\n",
    "\n",
    "3. **Parallel Processing**: Neuromorphic platforms employ parallel processing architectures that can efficiently handle the massive amount of data generated by event-driven cameras. This parallelism is essential for processing complex and time-sensitive tasks, such as object recognition, tracking, and scene understanding.\n",
    "\n",
    "4. **Adaptive and Learning Capabilities**: Inspired by the plasticity of the human brain, neuromorphic platforms can exhibit adaptive and learning capabilities. By incorporating neural-inspired algorithms and circuitry, these platforms can adapt to changing environments, learn from data, and improve their performance over time.\n",
    "\n",
    "5. **Robustness to Noise**: Event-driven cameras and neuromorphic platforms are inherently robust to noise and variations in lighting conditions. They can handle challenging environments with dynamic lighting changes, making them suitable for applications in robotics, vision-based control, and surveillance.\n",
    "\n",
    "Overall, neuromorphic platforms, powered by event-driven sensing and event-driven cameras, offer a promising approach for addressing complex computational tasks with energy efficiency and real-time processing capabilities, closely mimicking the human brain's efficiency and adaptability in visual processing.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"Images/Slide29.png\" alt=\"image_description\" width=\"600\" height=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b995c9a-2da3-4b98-ab08-5285044e977a",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"Images/Slide30.png\" alt=\"image_description\" width=\"600\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### SpiNNaker\n",
    "\n",
    "SpiNNaker (Spiking Neural Network Architecture) is a massively parallel computing platform developed by the University of Manchester. It consists of custom-designed chips that simulate the behavior of spiking neural networks (SNNs). SNNs are a type of neural network that more closely resemble the information processing in the human brain.\n",
    "\n",
    "Key features of SpiNNaker include:\n",
    "\n",
    "1. **Parallel Processing**: SpiNNaker enables the execution of large-scale neural network simulations by distributing computational load across thousands of interconnected chips. This parallel architecture allows for efficient processing of spiking neural networks and real-time performance.\n",
    "\n",
    "2. **Event-Based Communication**: SpiNNaker utilizes an event-based communication mechanism, where neural spikes are communicated between computing nodes asynchronously. This event-driven approach is highly efficient and enables real-time processing of spiking neural networks.\n",
    "\n",
    "3. **Scalability**: SpiNNaker is highly scalable and can accommodate networks ranging from a few neurons to billions of neurons. Its modular architecture allows for easy expansion and customization based on the computational requirements of different applications.\n",
    "\n",
    "4. **Power Efficiency**: SpiNNaker is designed to be power-efficient. By leveraging specialized hardware and event-driven communication, it reduces power consumption compared to traditional computing systems.\n",
    "\n",
    "SpiNNaker has been used in various research domains, including neuroscience, robotics, and cognitive computing. Its flexibility, scalability, and real-time processing capabilities make it well-suited for applications involving large-scale neural simulations and spiking neural networks.\n",
    "\n",
    "### Loihi\n",
    "\n",
    "Loihi is a neuromorphic research chip developed by Intel Labs. It is designed to mimic the behavior of biological neural networks using spiking neurons and adaptive synapses. Key features of Loihi include:\n",
    "\n",
    "1. **Neuromorphic Architecture**: Loihi's architecture is inspired by the human brain, with a focus on biological realism. It incorporates on-chip learning, spike-based computation, and event-driven communication.\n",
    "\n",
    "2. **Adaptive Learning**: Loihi supports on-chip learning mechanisms, allowing the network to adapt and improve its performance over time. This capability enables autonomous learning and adaptation in real-time applications.\n",
    "\n",
    "3. **Energy Efficiency**: Loihi emphasizes energy efficiency, leveraging a combination of parallel processing and sparsity to minimize power consumption. It is designed to efficiently handle the computational requirements of large-scale spiking neural networks.\n",
    "\n",
    "Loihi has demonstrated promising results in areas such as pattern recognition, anomaly detection, and robotic control, offering a platform for exploring neuromorphic computing principles.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"Images/Slide31.png\" alt=\"image_description\" width=\"600\" height=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a2e98-87f4-4070-a6b2-90eff69142e1",
   "metadata": {},
   "source": [
    "## And now a mini tutorial to create a neural networkðŸŽ‰\n",
    "\n",
    "You will learn how to create a simple neuronal population and stimulate it on Brian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76c2c67-8f38-45b6-a350-a8dbfa8d6545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brian2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from brian2) (1.24.3)\n",
      "Requirement already satisfied: jinja2>=2.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from brian2) (3.1.2)\n",
      "Requirement already satisfied: sympy>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from brian2) (1.12)\n",
      "Requirement already satisfied: setuptools>=24.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from brian2) (65.5.0)\n",
      "Requirement already satisfied: pyparsing in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from brian2) (3.0.9)\n",
      "Requirement already satisfied: cython>=0.29 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from brian2) (0.29.35)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2>=2.7->brian2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy>=1.2->brian2) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install brian2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42d36e-6d44-4fa1-96c1-b58c5f17fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "\n",
    "start_scope()\n",
    "\n",
    "#parameters of the group of neurons\n",
    "N = 100 #number of neurons\n",
    "tau = 10*ms #parameter related to the exponential decay \n",
    "v0_max = 3.\n",
    "duration = 1000*ms #duration of the simulation\n",
    "\n",
    "eqs = '''\n",
    "dv/dt = (v0-v)/tau : 1 (unless refractory)\n",
    "v0 : 1\n",
    "'''\n",
    "#create a neruon group\n",
    "G = NeuronGroup(N, eqs, threshold='v>1', reset='v=0', refractory=5*ms, method='exact')\n",
    "#record the voltage membrane potential for the neurons\n",
    "M = SpikeMonitor(G)\n",
    "\n",
    "G.v0 = 'i*v0_max/(N-1)'\n",
    "\n",
    "#run simulation\n",
    "run(duration)\n",
    "\n",
    "#raster plot and firing rate of the population\n",
    "figure(figsize=(12,4))\n",
    "subplot(121)\n",
    "plot(M.t/ms, M.i, '.k')\n",
    "xlabel('Time (ms)')\n",
    "ylabel('Neuron index')\n",
    "subplot(122)\n",
    "plot(G.v0, M.count/duration)\n",
    "xlabel('v0')\n",
    "ylabel('Firing rate (sp/s)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a76e62-cab3-4160-8bab-ca77039faa86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5468de-c6df-4bf0-9e80-7ac0d8c38d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
